\chapter{Introduction}
\section{Motivation}
Writing parallel programms is not a trivial task. Parallel codes are hard to write because they usually written in low level languages with verbose and non-idiomatic decorations, hard to debug because machines where codes are written are usually different from machines where codes are intended to run and hard to maintain and reuse because even though the underlying algorithms is not changed, multiple version of parallel codes are needed to tackle various platform and evolution of architectures.

There're many on-going researches in helping programmers write correct parallel programs easily. A common approach is to develop a higher level language and compiles programms in this language to required parallel codes. There're many paradigms of the high-level languages to describe parallel programms (TODO add some examples other than arrows). An example is to use arrow terms \cite{braunArrowsParallelComputation2018} to describe dataflow implicitly and hence generate parallel codes.

The workflow of writing parallel codes has evolved from writing parallel codes in the target platform directly to writing programms in a high level languages designed for parallel computation and then compiling to the target platfrom. In this project, we present a method to improve the "backend" of parallel code generation by introducing a monadic domain specific language to be a bridge between high-level languages and target parallel low-level languages.

This specific languages needs to be general enough so that different paradigms of high-level language to describe parallel programs can be translated to this languages. It can be used to generate different parallel codes e.g. MPI \footnote{Message Passing Interface (MPI) is a standardized and portable message-passing standard designed by a group of researchers from academia and industry to function on a wide variety of parallel computing architectures \cite{MessagePassingInterface2018}.}, Cuda. Moreover, it can be interpreted with a simulator to aid debugging parallel programs.

With the help of this intermediate languages, the implementation complexity is reduced from $O(M * N)$, where each of the M high-level languages needs to implement N compilers to generate parallel codes in N different platforms, to $O(M + N)$, where each high level language implements a translation rule to the intermediate languages and intermediate languages implements N compilers to generate different target languages.

In addition, it couples with multiparty session type (MPST) \cite{coppoGentleIntroductionMultiparty2015}. It takes advantages of properties of MPST to enable aggresive optimisation but ensuring code correctness and allow more meaningful static analysis (TODO add more examples of possible kinds of static analysis).

\section{Objectives}
(TODO Need refinement and adding concrete details)
\begin{enumerate}
\item \textbf{Design}: Design the intermediate languages, argue its generanality and build its connection with MPST. 
\item \textbf{Tranlsation}: Give an example of how to translate high level languages to our languages. 
\item \textbf{Simulator}: Build a simulator to prove the correctness of code generation and act as a playground for experiments.
\item \textbf{Code generation}: Generate parallel codes in C.
\item \textbf{Static Analysis}: Give an example of how MPST can be used in improving the generated codes.
\end{enumerate}
\section{Challenges}
TODO
% \section{Contributions}